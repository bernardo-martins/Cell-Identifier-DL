# -*- coding: utf-8 -*-
"""ProjetoAP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10UoTIB2bzapnw548-w_0haijQUUA0yLa

"""

"""Imports"""

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import BatchNormalization,Conv2D, MaxPooling2D, concatenate, AveragePooling2D
from tensorflow.keras.layers import Activation, Flatten, Dropout, Dense, Input, UpSampling2D, Reshape
from tp1_utils import load_data, compare_masks, overlay_masks

import numpy as np
from tensorflow import keras

import matplotlib.pyplot as plt

"""Ploting"""

def plot_acc_loss(history, batch_size, accuracy_type, val_accuracy_type):
  plt.plot(history.history[accuracy_type])
  plt.plot(history.history[val_accuracy_type])
  plt.title('Model Accuracy ' + str(batch_size))
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['train', 'valid'], loc='upper left')
  plt.show()


  # summarize history for loss
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('Model Loss ' + str(batch_size))
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'valid'], loc='upper left')
  plt.show()

"""Multi-Class"""

def create_model_class():

    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation = 'relu', padding="same", input_shape=(64,64,3)))
    model.add(Conv2D(32, (3, 3), activation = 'relu', padding="same"))

    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(64, (3, 3), activation = 'relu', padding="same"))
    model.add(Conv2D(64, (3, 3), activation = 'relu', padding="same"))
    
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(128, (3, 3), activation = 'relu', padding="same"))
    model.add(Conv2D(128, (3, 3), activation = 'relu', padding="same"))

    model.add(MaxPooling2D(pool_size=(2, 2)))
    
    model.add(Flatten())
    model.add(Dense(256, kernel_initializer="random_normal", bias_initializer="zeros"))
    model.add(Activation("relu"))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(10, kernel_initializer='random_normal', bias_initializer='zeros'))
    model.add(Activation("softmax"))
 
    return model
 
def run_multiclass():
  LR = 0.001
  MOMENTUM = 0.9
  EPOCHS = 35

  data = load_data()
  opt = keras.optimizers.Adam(learning_rate=LR,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07)


  batch_vals = [i for i in range(30, 150, 5)]

  accuracies_per_batch = []

  for batch in batch_vals:
    model = create_model_class()
    model.summary()
    print('-------------------------------------')
    model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

    history = model.fit(data['train_X'][:3500], data['train_classes'][:3500], validation_data=(data['train_X'][3500::], data['train_classes'][3500::]), batch_size=batch, epochs=EPOCHS, shuffle = True)

    loss, accuracy = model.evaluate(data['test_X'], data['test_classes'], verbose=0)
    accuracies_per_batch.append((np.sum(history.history['val_accuracy'])/len(history.history['val_accuracy'])))

    plot_acc_loss(history, batch, 'accuracy', 'val_accuracy')


  plt.plot(batch_vals, accuracies_per_batch, 'o', color='black')

  plt.xlabel('Batch values')
  plt.ylabel('Accuracy')
  plt.title('Accuracy per batch size')
  plt.legend(['train', 'valid'], loc='upper left')
  plt.show()

run_multiclass()

"""Multi-Label"""

def create_model_label():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation = 'relu', padding="same", input_shape=(64,64,3)))
    model.add(Conv2D(32, (3, 3), activation = 'relu', padding="same"))

    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(64, (3, 3), activation = 'relu', padding="same"))
    model.add(Conv2D(64, (3, 3), activation = 'relu', padding="same"))
    
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(128, (3, 3), activation = 'relu', padding="same"))
    model.add(Conv2D(128, (3, 3), activation = 'relu', padding="same"))

    model.add(MaxPooling2D(pool_size=(2, 2)))
    
    model.add(Flatten())
    model.add(Dense(256, kernel_initializer="random_normal", bias_initializer="zeros"))
    model.add(Activation("relu"))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(10, kernel_initializer='random_normal', bias_initializer='zeros'))
    
    model.add(Activation("sigmoid"))
    
    return model

def run_multilabel():
  LR = 0.0001
  MOMENTUM = 0.9
  EPOCHS = 50
  BATCH_SIZE = 50
  data = load_data()

  opt = keras.optimizers.Adam(learning_rate=LR,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07)

  #shuffle nos dado


  model = create_model_label()
  model.summary()

  model.compile(loss="binary_crossentropy", optimizer=opt,metrics=["binary_accuracy"])

  history = model.fit(data['train_X'][:3500], data['train_labels'][:3500], validation_data=(data['train_X'][3500::], data['train_labels'][3500::]),batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle = True)

  loss, accuracy = model.evaluate(data['test_X'], data['test_classes'], verbose=0)


  plot_acc_loss(history, BATCH_SIZE, 'binary_accuracy', 'val_binary_accuracy')

run_multilabel()

"""Segmentation Model"""

def create_model_segmentation():
    in1 = Input(shape=(64, 64, 3))
    #--------------------------
    conv1 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(in1)
    conv1 = Dropout(0.5)(conv1)
    pool1 = MaxPooling2D((2, 2))(conv1)
    #--------------------------
    conv2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool1)
    conv2 = Dropout(0.5)(conv2)
    pool2 = MaxPooling2D((2, 2))(conv2)

    up2 = UpSampling2D((2, 2))(pool2)
    conv7 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(up2)
    conv7 = Dropout(0.5)(conv7)
    up3 = UpSampling2D((2, 2))(conv7)
    segmentation = Conv2D(1, (1, 1), activation='sigmoid', name='seg')(up3)

    model = Model(inputs=[in1], outputs=[segmentation])
    model.summary()
    return model



def run_segmentation():
  LR = 0.01
  MOMENTUM = 0.9
  EPOCHS = 30
  BATCH_SIZE = 50

  batch_plot = []
  batch_vals = [i for i in range(50, 55, 5)]

  data = load_data()
  #for batch_val in batch_vals:
  opt = keras.optimizers.Adam(learning_rate=LR,
      beta_1=0.9,
      beta_2=0.999,
      epsilon=1e-07)

  #shuffle nos dado
  model = create_model_segmentation()

  model.compile(loss="binary_crossentropy", optimizer=opt,metrics=["binary_accuracy"])

  history = model.fit(data['train_X'][:3500], data['train_masks'][:3500], validation_data=(data['train_X'][3500::], data['train_masks'][3500::]),batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle = True)

  loss, accuracy = model.evaluate(data['test_X'], data['test_masks'], verbose=0)
  #batch_plot.append(accuracy)

  predicts = model.predict(data['test_X'])
  compare_masks('test_compare.png',data['test_masks'],predicts)
  overlay_masks('test_overlay.png',data['test_X'],predicts)

  plot_acc_loss(history, BATCH_SIZE, 'binary_accuracy', 'val_binary_accuracy' )

run_segmentation()